{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Cfn_hTVJfK"
      },
      "source": [
        "*Credits: materials from this notebook belong to YSDA [Practical DL](https://github.com/yandexdataschool/Practical_DL) course. Special thanks for making them available online.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPPbYpwHVJfN"
      },
      "source": [
        "# Lab assignment â„–1, part 1\n",
        "\n",
        "This lab assignment consists of several parts. You are supposed to make some transformations, train some models, estimate the quality of the models and explain your results.\n",
        "\n",
        "Several comments:\n",
        "* Don't hesitate to ask questions, it's a good practice.\n",
        "* No private/public sharing, please. The copied assignments will be graded with 0 points.\n",
        "* Blocks of this lab will be graded separately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F0EsfgxVJfO"
      },
      "source": [
        "## 1. Matrix differentiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLdvHRh5VJfO"
      },
      "source": [
        "Since it easy to google every task please please please try to undestand what's going on. The \"just answer\" thing will be not counted, make sure to present derivation of your solution. It is absolutely OK if you found an answer on web then just exercise in $\\LaTeX$ copying it into here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoSd_JQIVJfO"
      },
      "source": [
        "Useful links:\n",
        "[1](http://www.machinelearning.ru/wiki/images/2/2a/Matrix-Gauss.pdf)\n",
        "[2](http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuP60fZbVJfP"
      },
      "source": [
        "## ex. 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UZLdxdGVJfP"
      },
      "source": [
        "$$  \n",
        "y = x^Tx,  \\quad x \\in \\mathbb{R}^N\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02syDRkmVJfQ"
      },
      "source": [
        "$$\n",
        "\\frac{dy}{dx} =\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "$$\\frac{\\partial}{\\partial x_i}\\sum_{i=1}^nx_i^2=2x_i$$\n",
        "\n",
        "$$\n",
        "\\frac{dy}{dx} = 2x\n",
        "$$"
      ],
      "metadata": {
        "id": "_JrkFL2LWr1w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "Gvravs1MVJfQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5F3kF6aVJfR"
      },
      "source": [
        "## ex. 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEbt2LSuVJfS"
      },
      "source": [
        "$$ y = tr(AB) \\quad A,B \\in \\mathbb{R}^{N \\times N} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6aLJtKrVJfS"
      },
      "source": [
        "$$\n",
        "\\frac{dy}{dA} =\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "$$\\frac{\\partial}{\\partial a_{ij}}tr(AB)=\\frac{\\partial}{\\partial a_{ij}}\\left(\\sum_{j=1}^na_{1j}b_{j1}+\\dots+\\sum_{j=1}^na_{nj}b_{jn}\\right)=b_{ji}$$\n",
        "\n",
        "$$\n",
        "\\frac{dy}{dA} =B^T\n",
        "$$"
      ],
      "metadata": {
        "id": "5XKVIDwpXcXx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "dIoSfyi0VJfS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA5CBOP_VJfS"
      },
      "source": [
        "## ex. 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBlRKBBsVJfT"
      },
      "source": [
        "$$  \n",
        "y = x^TAc , \\quad A\\in \\mathbb{R}^{N \\times N}, x\\in \\mathbb{R}^{N}, c\\in \\mathbb{R}^{N}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZd7EjOlVJfT"
      },
      "source": [
        "$$\n",
        "\\frac{dy}{dx} =\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "$$y = x^TAc = \\sum_{i=1}^nx_ia_{1i}c_1+\\dots+\\sum_{i=1}^nx_ia_{ni}c_n.$$\n",
        "\n",
        "\n",
        "$$\\frac{\\partial}{\\partial x_i}y=a_{1i}c_1+\\dots+a_{ni}c_n$$\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{dy}{dx} =A^Tc\n",
        "$$"
      ],
      "metadata": {
        "id": "agoYLsGCZGr7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEpBRw8AVJfT"
      },
      "source": [
        "$$\n",
        "\\frac{dy}{dA} =\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\frac{\\partial}{\\partial a_{ij}}y=\\frac{\\partial}{\\partial a_{ij}}\\sum_{j=1}^nx_ja_{ij}c_i=x_jc_i$$\n",
        "\n",
        "$$\n",
        "\\frac{dy}{dA} = c^Tx\n",
        "$$"
      ],
      "metadata": {
        "id": "HZZw7dmTZIP_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYQfIQVEVJfU"
      },
      "source": [
        "Hint for the latter (one of the ways): use *ex. 2* result and the fact\n",
        "$$\n",
        "tr(ABC) = tr (CAB)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "0-REbm3IVJfU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DYhU-g7VJfU"
      },
      "source": [
        "## ex. 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzc5QJH5VJfV"
      },
      "source": [
        "Classic matrix factorization example. Given matrix $X$ you need to find $A$, $S$ to approximate $X$. This can be done by simple gradient descent iteratively alternating $A$ and $S$ updates.\n",
        "$$\n",
        "J = || X - AS ||_F^2  , \\quad A\\in \\mathbb{R}^{N \\times R} , \\quad S\\in \\mathbb{R}^{R \\times M}\n",
        "$$\n",
        "$$\n",
        "\\frac{dJ}{dS} = ?\n",
        "$$\n",
        "\n",
        "You may use one of the following approaches:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9daztyeKVJfV"
      },
      "source": [
        "#### First approach\n",
        "Using ex.2 and the fact:\n",
        "$$\n",
        "|| X ||_F^2 = tr(XX^T)\n",
        "$$\n",
        "it is easy to derive gradients (you can find it in one of the refs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b32crOeVJfV"
      },
      "source": [
        "#### Second approach\n",
        "You can use *slightly different techniques* if they suits you. Take a look at this derivation:\n",
        "<img src=\"https://github.com/girafe-ai/ml-course/blob/23f_basic/homeworks/lab01_ml_pipeline/grad.png?raw=1\">\n",
        "(excerpt from [Handbook of blind source separation, Jutten, page 517](https://books.google.ru/books?id=PTbj03bYH6kC&printsec=frontcover&dq=Handbook+of+Blind+Source+Separation&hl=en&sa=X&ved=0ahUKEwi-q_apiJDLAhULvXIKHVXJDWcQ6AEIHDAA#v=onepage&q=Handbook%20of%20Blind%20Source%20Separation&f=false), open for better picture)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMtv5UiDVJfW"
      },
      "source": [
        "#### Third approach\n",
        "And finally we can use chain rule!\n",
        "let $ F = AS $\n",
        "\n",
        "**Find**\n",
        "$$\n",
        "\\frac{dJ}{dF} =  \n",
        "$$\n",
        "and\n",
        "$$\n",
        "\\frac{dF}{dS} =  \n",
        "$$\n",
        "(the shape should be $ NM \\times RM )$.\n",
        "\n",
        "Now it is easy do get desired gradients:\n",
        "$$\n",
        "\\frac{dJ}{dS} =  \n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "J = || X - AS ||_F^2 = tr((X - AS)(X^T - S^TA^T)) =  tr(ASS^TA^T) - tr(ASX^T) + tr(XX^T) - tr(XS^TA^T)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{d(tr(ASX^T))}{dF} = X\n",
        "$$\n",
        "$$\n",
        "\\frac{d(tr(XS^TA^T))}{dF} = X\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{d(tr(ASS^TA^T))}{dF} = 2AS\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{d(tr(XX^T))}{dF} = 0\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{dJ}{dF} = 2(AS - X)\n",
        "$$\n",
        "and\n",
        "\n",
        "$$\n",
        "\\frac{dF}{dS} = A^T\n",
        "$$\n",
        "\n",
        "(the shape should be $ NM \\times RM )$.\n",
        "\n",
        "Now it is easy do get desired gradients:\n",
        "$$\n",
        "\\frac{dJ}{dS} = \\frac{dJ}{dF}\\frac{dF}{dS} = 2A^T(AS-X)\n",
        "$$"
      ],
      "metadata": {
        "id": "367YPWPyjLcf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mXz2HzjMjKQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "dQBELEROVJfW"
      },
      "source": [
        "## 2. kNN questions\n",
        "Here come the questions from the assignment0_01. Please, refer to the assignment0_01 to get the context of the questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rop6-An1VJfW"
      },
      "source": [
        "### Question 1\n",
        "\n",
        "Notice the structured patterns in the distance matrix, where some rows or columns are visible brighter. (Note that with the default color scheme black indicates low distances while white indicates high distances.)\n",
        "\n",
        "- What in the data is the cause behind the distinctly bright rows?\n",
        "- What causes the columns?\n",
        "\n",
        "*Your Answer:*\n",
        "\n",
        "-The existence of bright rows indicates significant disparities between the points representing a specific test point and all the training points, resulting in a diminished predictive quality within our model.\n",
        "-In contrast, bright columns imply notable differences between the point representing an individual training data point and all the test points.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAvoVoR1VJfX"
      },
      "source": [
        "### Question 2\n",
        "\n",
        "We can also use other distance metrics such as L1 distance.\n",
        "For pixel values $p_{ij}^{(k)}$ at location $(i,j)$ of some image $I_k$,\n",
        "\n",
        "the mean $\\mu$ across all pixels over all images is $$\\mu=\\frac{1}{nhw}\\sum_{k=1}^n\\sum_{i=1}^{h}\\sum_{j=1}^{w}p_{ij}^{(k)}$$\n",
        "And the pixel-wise mean $\\mu_{ij}$ across all images is\n",
        "$$\\mu_{ij}=\\frac{1}{n}\\sum_{k=1}^np_{ij}^{(k)}.$$\n",
        "The general standard deviation $\\sigma$ and pixel-wise standard deviation $\\sigma_{ij}$ is defined similarly.\n",
        "\n",
        "Which of the following preprocessing steps will not change the performance of a Nearest Neighbor classifier that uses L1 distance? Select all that apply.\n",
        "1. Subtracting the mean $\\mu$ ($\\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\\mu$.)\n",
        "2. Subtracting the per pixel mean $\\mu_{ij}$  ($\\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\\mu_{ij}$.)\n",
        "3. Subtracting the mean $\\mu$ and dividing by the standard deviation $\\sigma$.\n",
        "4. Subtracting the pixel-wise mean $\\mu_{ij}$ and dividing by the pixel-wise standard deviation $\\sigma_{ij}$.\n",
        "5. Rotating the coordinate axes of the data.\n",
        "\n",
        "*Your Answer:*\n",
        "1. NO\n",
        "2. NO\n",
        "3. NO\n",
        "4. YES\n",
        "5. YES\n",
        "\n",
        "*Your Explanation:*\n",
        "\n",
        "Sure, here's the translation of the provided text from Russian to English:\n",
        "\n",
        "1. Changing the feature values by a constant will not change the distances between them. Therefore, KNN relying only on distances between objects will not change its result.\n",
        "\n",
        "2. Similarly. Each pixel is an individual feature. Changing the values of any feature by a constant will not change the distances between features.\n",
        "\n",
        "3. Changing the values of any feature by a constant will not change the distances between these features. However, if we divide the values of all features by a constant Ïƒ, the distance between any two objects will decrease by Ïƒ times. This means that the ordering of the features from closest to farthest will not change for any feature. The kNN results will remain the same.\n",
        "\n",
        "4. When calculating the L1 metric, the distances between objects are summarized for each attribute. At the specified changes, the distances between objects will be reduced by a certain coefficient determined separately for each attribute. This transformation does not guarantee that the order of objects from nearest to farthest will remain unchanged. For example, consider a two-pixel image. One is at (0, 0), the second at (6, 6), the third at (14, 0), and a few more. The variance for the first pixel is 2 and for the second pixel is 1. The second pixel is closer to the first pixel than the third pixel (normalized to L1). However, after transforming the feature vectors, they become (0, 0), (3, 6), (7, 0) respectively. Now the third is closer to the first than the second. The answer in 1-NN has changed. Similarly, the answer in 5-NN can be changed. Thus, the results of the algorithm can change.\n",
        "\n",
        "5. When the image is rotated, the rotation of coordinate axes only swaps the signs. At this transformation the value of norm L1 does not change. Consequently, the distances will not change. The work kNN remains unchanged.\n",
        "If rotation takes place in the feature space of the classifier L1, the order of distances from one object to others may change:\n",
        "$x_0=(0,0),~y_0=(5,0),~z_0=(4,3)$. When rotating by an angle $\\alpha=arctg(4/3):$ $x_1=(0,0),~y_1=(3,4),~z_1=(0,5)$.\n",
        "$$\n",
        "||y_0-x_0||_1=5 < ||z_0-x_0||_1=7, ||y_1-x_1||_1=7 > ||z_1-x_1||_1=5.\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzCRbdd5VJfX"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "Which of the following statements about $k$-Nearest Neighbor ($k$-NN) are true in a classification setting, and for all $k$? Select all that apply.\n",
        "1. The decision boundary (hyperplane between classes in feature space) of the k-NN classifier is linear.\n",
        "2. The training error of a 1-NN will always be lower than that of 5-NN.\n",
        "3. The test error of a 1-NN will always be lower than that of a 5-NN.\n",
        "4. The time needed to classify a test example with the k-NN classifier grows with the size of the training set.\n",
        "5. None of the above.\n",
        "\n",
        "*Your Answer:*\n",
        "1. NO\n",
        "2. YES\n",
        "3. NO\n",
        "4. YES\n",
        "5. NO\n",
        "\n",
        "*Your Explanation:*\n",
        "1. **NO**: The decision boundary of a k-NN classifier is not necessarily linear. It can be highly nonlinear and is determined by the distribution of the training data.\n",
        "2. **YES**: The training error of 1-NNs is usually lower than that of 5-NNs because 1-NNs tend to memorize the training data (which may lead to overfitting).\n",
        "3. **NO**: The test error for 1-NN will not always be lower than that of 5-NN. 1-NC may perform well on training data, but may have higher test error due to overfitting or excessive sensitivity to noise.\n",
        "4. **YES**: The time required to classify a test case using a k-NN classifier increases with the size of the training set because of the computation required to find nearest neighbors.\n",
        "5. **NO**: As stated above, several statements are true.\n",
        "\n",
        "\n",
        "The k-NN decision boundary is not necessarily linear, 1-NN usually has smaller training error than 5-NN, but is not guaranteed for testing error. The classification time increases with the size of the training set.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mipt",
      "language": "python",
      "name": "mipt"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}